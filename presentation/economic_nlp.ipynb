{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Aim](#aim)\n",
    "* [Approach](#approach)\n",
    "    * [Global Knowledge Graph](#gkg)\n",
    "* [Phase 1](#phase1)\n",
    "    * [Tone of Articles](#tone_of_articles)\n",
    "    * [Consumer Sentiment](#consumer_sentiment)\n",
    "* [Phase 2](#phase2)\n",
    "    * [Correlation](#correlation)\n",
    "    * [Topic Modeling](#topic_modeling)\n",
    "* [Phase 3](#phase3)\n",
    "    * [Macroeconomic Indicator Forecast](#gdp_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim <a class=\"anchor\" id=\"aim\"></a>\n",
    "The aim of this project is explore how alternative data could help our economic forecasts. One idea that has been considered was the scraping of newspapers, blogs etc for attitudes to things that change sentiment of the populus and subsequently and indirectly, the economy. \n",
    "\n",
    "## Approach <a class=\"anchor\" id=\"approach\"></a>\n",
    "To test this theory, the team conducted some analysis to explore topics around COVID 19. In particular, we have access to a public dataset courtest of https://www.gdeltproject.org/. The GDELT database monitors print, broadcast, and web news media in over 100 languages from across every country in the world to keep continually updated on breaking developments anywhere on the planet. Its historical archives stretch back to January 1, 1979 and update every 15 minutes. Through its ability to leverage the world's collective news media, GDELT moves beyond the focus of the Western media towards a far more global perspective on what's happening and how the world is feeling about it.\n",
    "\n",
    "### Global Knowledge Graph <a class=\"anchor\" id=\"gkg\"></a>\n",
    "One particular dataset within GDELT of interest, is the Global Knowledge Graph. Not only do we have the links to the articles, but we have various useful enrichments:\n",
    "\n",
    "Overview\n",
    "\n",
    "- Around 1bn rows\n",
    "\n",
    "- Updated every 15 mins\n",
    "\n",
    "- Each entry is a news article\n",
    "\n",
    "- Detailed information on every person, location, number and theme mentioned in an article\n",
    "\n",
    "Can explore the GDELT database in Google Big Query here: https://console.cloud.google.com/bigquery?authuser=1&project=goldenfleece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 <a class=\"anchor\" id=\"phase1\"></a>\n",
    "\n",
    "### Tone of Articles <a class=\"anchor\" id=\"tone_of_articles\"></a>\n",
    "\n",
    "We extracted the tone data from GKG to understand the general sentiment of all the articles across the year.\n",
    "\n",
    "![Tone of Articles](img/articles_tone.png)\n",
    "\n",
    "the task is to mine the GDELT database for all news articles related to the mentions of “vaccine” and analyse if we think the sentiment is positive (vaccine will work, life back to normal etc), negative (side effects will be awful, not proven, not safe etc) to see how possible it is. This could be explored via a multitude of approaches, such as Bag of words Models, unsupervised LDA topic modelling and associated sentiment analysis to understand the context of the articles that display positive vs negative sentiment. Sentiment can also change over time, so there is an opporunity to explore this also.\n",
    "\n",
    "Link to dashboard: https://datastudio.google.com/reporting/50b45093-253e-4be0-b917-b5c9bf2d0cef/page/vIRqB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Sentiment <a class=\"anchor\" id=\"consumer_sentiment\"></a>\n",
    "\n",
    "So far we have conducted a lot of research around the supply side of news, but what about the response to it?\n",
    "\n",
    "#### Reddit Comments\n",
    "We obtained access to reddit comments for the same time period, but this time focused on April and Novembver, courtesy of https://pushshift.io/\n",
    "\n",
    "Sentiment was calculated using subjectivity * polairty, where subjectivity ranges between 0-1 and polarity from -1 and +1. Essentially, very personal positive comments tends towards score of +1. Sentiment scores are the average of comments on that day.\n",
    "\n",
    "![Consumer Sentiment](img/consumer_sentiment.png)\n",
    "\n",
    "Looks like the overall sentiment for November is generally slightly higher!\n",
    "\n",
    " \n",
    "Looking at the most negative comments in April:\n",
    "\n",
    "![Negative comment 1](img/negative_comment1.png)\n",
    "\n",
    "![Negative comment 2](img/negative_comment2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Then looking at some positive comments in November - some politically linked, comments on shares. \n",
    "![Positive comment 1](img/positive_comment1.png)\n",
    "\n",
    "![Positive comment 2](img/positive_comment2.png)\n",
    "\n",
    "![Positive comment 3](img/positive_comment3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 <a class=\"anchor\" id=\"phase2\"></a>\n",
    "\n",
    "### Stock Market Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we know how to mine data for sentiment, but what about trying to get a signal from it? We explore ways in which we can leverage the use of topic models to infer better understanding and attempt to correlate to the stock market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling <a class=\"anchor\" id=\"topic_modeling\"></a>\n",
    "\n",
    "Each news article in GDELT dataset was tagged with a variety of themes. The themes are taxonomy classifications from a number of different organisations including UN and World Bank. \n",
    "\n",
    ">There are thousands of different GKG themes, but no complete list seems to be provided anywhere. Probably the most comprehensive list of themes can be found within the documentation section of the GDELT project website. The list currently includes over 2,500 different themes. Among these themes are around 2,200 themes from the World Bank Taxonomy.\n",
    "\n",
    "We can manually sample some news articles and extract the economy themes like the ones below but this is not really efficient.\n",
    "\n",
    "- TAX_DISEASE_CORONAVIRUS\n",
    "- HEALTH_VACCINATIONS\n",
    "- EPU_GOVERNMENT_POLICY\n",
    "- EPU_POLICY_BUDGET\n",
    "- EPU_ECONOMY\n",
    "- EPU_ECONOMY_DEBT\n",
    "- ECON_STOCKMARKET\n",
    "- ECON_TAXATION\n",
    "- UNEMPLOYMENT\n",
    "- PROTEST\n",
    "\n",
    "Therefore, we apply Latent Dirichlet Allocation (LDA) topic modeling, which is a type of unsupervised learning to identify topics in a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 <a class=\"anchor\" id=\"phase3\"></a>\n",
    "\n",
    "### Macroeconomic Indicator Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So! we had some with topic modelling, but what else can we do?\n",
    "\n",
    "The bank of england released a paper called Making text count: economic forecasting\n",
    "using newspaper text drawn from three popular UK newspapers that collectively represent\n",
    "UK newspaper readership in terms of political perspective and editorial style\n",
    "\n",
    "The found that Incorporating text into forecasts by combining counts of terms with supervised machine\n",
    "learning delivers the best forecast improvements both in marginal terms and relative to existing text-based\n",
    "methods. These improvements are most pronounced during periods of economic stress when, arguably,\n",
    "forecasts matter most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. We constructed a term frequency model using word lists such as this: https://sraf.nd.edu/textual-analysis/resources/\n",
    "\n",
    "2. Build an ngram dataset covering all of the speech to text transcriptions of BBC news, bloomberg, CNBC and CNN over the last 10 years\n",
    "\n",
    "3. We then transformed the dataset and aligned it to timeseries of various economic indicatoers"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
